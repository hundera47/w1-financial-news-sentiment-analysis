{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 stage 2 \n",
    "#### Text Analysis(Sentiment analysis & Topic Modeling):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Necessary Libraries\n",
    "\n",
    "In this step, we import the libraries required for text analysis, including:\n",
    "- **Data Manipulation and Visualization:** `pandas`, `numpy`, `matplotlib`, and `seaborn`.\n",
    "- **Text Analysis and Sentiment Analysis:** `nltk` and `textblob`.\n",
    "- **Text Vectorization:** `scikit-learn`.\n",
    "- **Topic Modeling:** `spacy` and `gensim`.\n",
    "\n",
    "Ensure all libraries are installed and necessary corpora are downloaded. Restart the Jupyter notebook kernel after installing new packages and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Download NLTK data if not already present\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model for spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Please install it using 'python -m spacy download en_core_web_sm'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare the Data\n",
    "\n",
    "In this step, we will prepare our dataset for sentiment analysis and topic modeling. The process includes:\n",
    "1. **Loading the Data:** Read the financial news dataset into a pandas DataFrame.\n",
    "2. **Data Cleaning:** Handle any missing or inconsistent data.\n",
    "3. **Text Preprocessing:** Tokenize the text, remove stop words, and perform other preprocessing tasks.\n",
    "\n",
    "Ensure you have followed Step 1 and imported the necessary libraries before proceeding with this step.\n",
    "\n",
    "## 2.1 Load the Data\n",
    "\n",
    "We will start by loading the dataset from the `data` folder into a pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/raw_analyst_ratings.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Check for Missing Values\n",
    "\n",
    "We need to check for any missing values in the dataset. Handling missing values is crucial for accurate analysis. We'll identify any missing values and decide how to handle them (e.g., removing rows, imputing values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text_spacy(text):\n",
    "    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_stop]  # Remove punctuation and stop words\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the function to the headlines\n",
    "df['cleaned_headline'] = df['headline'].apply(clean_text_spacy)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "df[['headline', 'cleaned_headline']].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
